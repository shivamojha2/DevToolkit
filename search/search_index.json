{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DevToolkit","text":"<p>DevToolkit is a Python library that provides a simple and consistent interface for interacting with various Large Language Model (LLM) APIs. Currently, it supports OpenAI-compatible APIs &amp; Bedrock APIs and includes features for text completions, chat completions, vision requests, and batch processing.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Text Completions: Generate text completions from prompts</li> <li>Chat Completions: Interact with models using a chat-like interface</li> <li>Vision API: Process images and generate text descriptions</li> <li>Batch Processing: Process multiple requests efficiently</li> <li>Streaming Support: Get responses token by token</li> <li>Guided JSON Output: Enforce structured output formats</li> <li>Error Handling: Comprehensive error handling and reporting</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>API Reference</li> <li>OpenAI Examples</li> <li>Bedrock Examples</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the license included in the repository.</p>"},{"location":"api/batch-processing/","title":"Batch Processing","text":"<p>The batch processing API allows you to efficiently process multiple prompts in parallel. This is useful for processing large datasets or handling multiple requests simultaneously.</p>"},{"location":"api/batch-processing/#basic-usage","title":"Basic Usage","text":"<pre><code>from gen_ai.factory import create_client\n\n# Create a client\nclient = create_client(\n    provider=\"openai\",\n    api_key=\"your-api-key\",\n    endpoint=\"https://api.example.com/v1\",\n    model_name=\"gpt-3.5-turbo\"\n)\n\n# Process multiple prompts\nprompts = [\n    \"List 3 benefits of cloud computing.\",\n    \"What is the capital of France?\",\n    \"Provide a short definition of machine learning.\"\n]\n\nresponses = client.run_completions_batch(\n    prompts=prompts,\n    max_tokens=100\n)\n\n# Print results\nfor prompt, response in zip(prompts, responses):\n    print(f\"\\nPrompt: {prompt}\")\n    print(f\"Response: {response}\")\n</code></pre>"},{"location":"api/batch-processing/#parameters","title":"Parameters","text":""},{"location":"api/batch-processing/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>prompts</code>: List of prompt texts to process</li> </ul>"},{"location":"api/batch-processing/#optional-parameters","title":"Optional Parameters","text":"<ul> <li><code>max_tokens</code>: Maximum number of tokens to generate (default: 256)</li> <li><code>temperature</code>: Controls randomness (0.0 to 1.0, default: 0)</li> <li><code>top_p</code>: Controls diversity via nucleus sampling (default: 1.0)</li> <li><code>timeout</code>: Request timeout in seconds (default: 60)</li> <li><code>concurrent</code>: Whether to process requests concurrently (default: True)</li> <li><code>max_concurrent</code>: Maximum number of concurrent requests (default: 5)</li> <li><code>return_error</code>: Return error details along with the results</li> </ul>"},{"location":"api/batch-processing/#examples","title":"Examples","text":""},{"location":"api/batch-processing/#basic-batch-processing","title":"Basic Batch Processing","text":"<pre><code>prompts = [\n    \"Write a haiku about spring.\",\n    \"Write a haiku about summer.\",\n    \"Write a haiku about autumn.\",\n    \"Write a haiku about winter.\"\n]\n\nresponses = client.run_completions_batch(\n    prompts=prompts,\n    max_tokens=50,\n    temperature=0.7\n)\n</code></pre>"},{"location":"api/batch-processing/#error-handling","title":"Error Handling","text":"<pre><code>responses, errors = client.run_completions_batch(\n    prompts=prompts,\n    return_error=True\n)\n\nfor i, (response, error) in enumerate(zip(responses, errors)):\n    print(f\"\\nPrompt {i+1}:\")\n    if error:\n        print(f\"Error: {error['message']}\")\n        print(f\"Suggestion: {error['suggestion']}\")\n    else:\n        print(f\"Response: {response}\")\n</code></pre>"},{"location":"api/batch-processing/#concurrent-processing","title":"Concurrent Processing","text":"<pre><code># Process 10 prompts with max 3 concurrent requests\nresponses = client.run_completions_batch(\n    prompts=prompts,\n    concurrent=True,\n    max_concurrent=3,\n    max_tokens=100\n)\n</code></pre>"},{"location":"api/batch-processing/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Text Completions</li> <li>Explore Chat Completions</li> <li>Check out the OpenAI Examples</li> </ul>"},{"location":"api/bedrock/","title":"AWS Bedrock Integration","text":""},{"location":"api/bedrock/#overview","title":"Overview","text":"<p>The AWS Bedrock integration provides a client for interacting with various foundation models available through AWS Bedrock.</p>"},{"location":"api/bedrock/#client-setup","title":"Client Setup","text":"<pre><code>from gen_ai.factory import create_client\n\nclient = create_client(\n    provider=\"bedrock\",\n    api_key=\"your_aws_access_key\",\n    model_name=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    region=\"us-east-1\",\n    secret_key=\"your_aws_secret_key\",\n    session_token=\"optional_session_token\"\n)\n</code></pre>"},{"location":"api/bedrock/#available-methods","title":"Available Methods","text":""},{"location":"api/bedrock/#chat-completions","title":"Chat Completions","text":"<pre><code>messages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\nresponse = client.run_chat_completions(messages)\n</code></pre>"},{"location":"api/bedrock/#streaming-chat-completions","title":"Streaming Chat Completions","text":"<pre><code>for chunk in client.run_chat_completions_streaming(messages):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"api/bedrock/#vision-api","title":"Vision API","text":"<pre><code>messages = [{\"role\": \"user\", \"content\": \"What's in this image?\"}]\nimage_paths = [\"path/to/image.jpg\"]\nresponse = client.run_chat_completions(messages, image_paths=image_paths)\n</code></pre>"},{"location":"api/bedrock/#message-format","title":"Message Format","text":"<ul> <li>Text messages: <code>{\"role\": \"user\", \"content\": \"message text\"}</code></li> <li>Multimodal messages: <code>{\"role\": \"user\", \"content\": [{\"text\": \"message text\"}, {\"image\": {...}}]}</code></li> </ul>"},{"location":"api/bedrock/#error-handling","title":"Error Handling","text":"<p>The client handles common AWS errors and provides detailed error messages for debugging.</p>"},{"location":"api/chat-completions/","title":"Chat Completions","text":"<p>The chat completions API allows you to interact with the model using a chat-like interface. This is useful for conversations, multi-turn interactions, and maintaining context.</p>"},{"location":"api/chat-completions/#basic-usage","title":"Basic Usage","text":"<pre><code>from gen_ai.factory import create_client\n\n# Create a client\nclient = create_client(\n    provider=\"openai\",\n    api_key=\"your-api-key\",\n    endpoint=\"https://api.example.com/v1\",\n    model_name=\"gpt-3.5-turbo\"\n)\n\n# Chat with the model\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What are the key benefits of modular code?\"}\n]\n\nresponse = client.run_chat_completions(\n    messages=messages,\n    max_tokens=150\n)\nprint(response)\n</code></pre>"},{"location":"api/chat-completions/#streaming","title":"Streaming","text":"<p>For real-time chat responses, use the streaming API:</p> <pre><code># Stream the response token by token\nfor token in client.run_chat_completions_stream(\n    messages=messages,\n    max_tokens=150\n):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"api/chat-completions/#message-format","title":"Message Format","text":"<p>Messages are formatted as a list of dictionaries with the following structure:</p> <pre><code>messages = [\n    {\n        \"role\": \"system\",  # System message to set behavior\n        \"content\": \"You are a helpful assistant.\"\n    },\n    {\n        \"role\": \"user\",    # User message\n        \"content\": \"Hello!\"\n    },\n    {\n        \"role\": \"assistant\",  # Assistant's previous response\n        \"content\": \"Hi! How can I help you?\"\n    }\n]\n</code></pre>"},{"location":"api/chat-completions/#parameters","title":"Parameters","text":""},{"location":"api/chat-completions/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>messages</code>: List of message dictionaries</li> </ul>"},{"location":"api/chat-completions/#optional-parameters","title":"Optional Parameters","text":"<ul> <li><code>max_tokens</code>: Maximum number of tokens to generate (default: 256)</li> <li><code>temperature</code>: Controls randomness (0.0 to 1.0, default: 0)</li> <li><code>top_p</code>: Controls diversity via nucleus sampling (default: 1.0)</li> <li><code>n</code>: Number of completions to generate (default: 1)</li> <li><code>timeout</code>: Request timeout in seconds (default: 30)</li> <li><code>guided_json</code>: JSON schema to enforce structured output</li> <li><code>return_error</code>: Return error details along with the result</li> </ul>"},{"location":"api/chat-completions/#examples","title":"Examples","text":""},{"location":"api/chat-completions/#basic-chat","title":"Basic Chat","text":"<pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What are the key benefits of modular code?\"}\n]\n\nresponse = client.run_chat_completions(\n    messages=messages,\n    max_tokens=150,\n    temperature=0.7\n)\n</code></pre>"},{"location":"api/chat-completions/#multi-turn-conversation","title":"Multi-turn Conversation","text":"<pre><code># First message\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is Python?\"}\n]\nresponse = client.run_chat_completions(messages=messages)\n\n# Add the response to the conversation\nmessages.append({\"role\": \"assistant\", \"content\": response})\n\n# Follow-up question\nmessages.append({\"role\": \"user\", \"content\": \"How does it compare to JavaScript?\"})\nresponse = client.run_chat_completions(messages=messages)\n</code></pre>"},{"location":"api/chat-completions/#structured-output","title":"Structured Output","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass CodeReview(BaseModel):\n    overall_score: float = Field(..., ge=0, le=10, description=\"Overall code quality score\")\n    strengths: list[str] = Field(..., description=\"List of code strengths\")\n    weaknesses: list[str] = Field(..., description=\"List of code weaknesses\")\n    suggestions: list[str] = Field(..., description=\"List of improvement suggestions\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an expert code reviewer.\"},\n    {\"role\": \"user\", \"content\": \"Review this code: def hello(): print('Hello')\"}\n]\n\nresponse = client.run_chat_completions(\n    messages=messages,\n    guided_json=CodeReview.model_json_schema(),\n    max_tokens=300\n)\n</code></pre>"},{"location":"api/chat-completions/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Text Completions</li> <li>Explore Vision API</li> <li>Check out the OpenAI Examples</li> </ul>"},{"location":"api/factory/","title":"Client Factory","text":""},{"location":"api/factory/#overview","title":"Overview","text":"<p>The client factory provides a unified interface for creating clients for different LLM providers.</p>"},{"location":"api/factory/#usage","title":"Usage","text":"<pre><code>from gen_ai.factory import create_client\n\n# Create OpenAI client\nopenai_client = create_client(\n    provider=\"openai\",\n    api_key=\"your_openai_key\",\n    model_name=\"gpt-4\",\n    endpoint=\"https://api.openai.com/v1\"\n)\n\n# Create Bedrock client\nbedrock_client = create_client(\n    provider=\"bedrock\",\n    api_key=\"your_aws_access_key\",\n    model_name=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    region=\"us-east-1\",\n    secret_key=\"your_aws_secret_key\"\n)\n</code></pre>"},{"location":"api/factory/#supported-providers","title":"Supported Providers","text":"<ul> <li>OpenAI</li> <li>AWS Bedrock</li> </ul>"},{"location":"api/factory/#common-parameters","title":"Common Parameters","text":"<ul> <li><code>provider</code>: The provider to use (\"openai\" or \"bedrock\")</li> <li><code>api_key</code>: API key for authentication</li> <li><code>model_name</code>: Name of the model to use</li> <li><code>endpoint</code>: API endpoint URL (for OpenAI)</li> <li><code>region</code>: AWS region (for Bedrock)</li> <li><code>secret_key</code>: AWS secret access key (for Bedrock)</li> <li><code>session_token</code>: AWS session token (for Bedrock)</li> </ul>"},{"location":"api/overview/","title":"API Overview","text":""},{"location":"api/overview/#supported-providers","title":"Supported Providers","text":"<ul> <li>OpenAI: Access to GPT models and other OpenAI services</li> <li>AWS Bedrock: Access to various foundation models including Claude, Llama, and others</li> </ul>"},{"location":"api/overview/#common-features","title":"Common Features","text":"<ul> <li>Text completions</li> <li>Chat completions</li> <li>Streaming responses</li> <li>Vision API support</li> <li>Batch processing</li> <li>Error handling</li> </ul>"},{"location":"api/overview/#provider-specific-features","title":"Provider-Specific Features","text":""},{"location":"api/overview/#openai","title":"OpenAI","text":"<ul> <li>Custom endpoints support</li> <li>Guided JSON output</li> <li>Batch processing</li> <li>Vision API with base64 image encoding</li> </ul>"},{"location":"api/overview/#aws-bedrock","title":"AWS Bedrock","text":"<ul> <li>Multiple model support (Claude, Llama, etc.)</li> <li>Raw image bytes support</li> <li>AWS credential management</li> <li>Region-specific endpoints</li> </ul>"},{"location":"api/text-completions/","title":"Text Completions","text":"<p>The text completions API allows you to generate text from a prompt. This is useful for tasks like text generation, summarization, and completion.</p>"},{"location":"api/text-completions/#basic-usage","title":"Basic Usage","text":"<pre><code>from gen_ai.factory import create_client\n\n# Create a client\nclient = create_client(\n    provider=\"openai\",\n    api_key=\"your-api-key\",\n    endpoint=\"https://api.example.com/v1\",\n    model_name=\"gpt-3.5-turbo\"\n)\n\n# Generate text\nresponse = client.run_completions(\n    prompt=\"Write a short poem about artificial intelligence.\",\n    max_tokens=100\n)\nprint(response)\n</code></pre>"},{"location":"api/text-completions/#streaming","title":"Streaming","text":"<p>For real-time text generation, use the streaming API:</p> <pre><code># Stream the response token by token\nfor token in client.run_completions_stream(\n    prompt=\"Write a short poem about artificial intelligence.\",\n    max_tokens=100\n):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"api/text-completions/#parameters","title":"Parameters","text":""},{"location":"api/text-completions/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>prompt</code>: The text prompt to generate from</li> </ul>"},{"location":"api/text-completions/#optional-parameters","title":"Optional Parameters","text":"<ul> <li><code>max_tokens</code>: Maximum number of tokens to generate (default: 256)</li> <li><code>temperature</code>: Controls randomness (0.0 to 1.0, default: 0)</li> <li><code>top_p</code>: Controls diversity via nucleus sampling (default: 1.0)</li> <li><code>n</code>: Number of completions to generate (default: 1)</li> <li><code>timeout</code>: Request timeout in seconds (default: 30)</li> <li><code>guided_json</code>: JSON schema to enforce structured output</li> <li><code>return_error</code>: Return error details along with the result</li> </ul>"},{"location":"api/text-completions/#examples","title":"Examples","text":""},{"location":"api/text-completions/#basic-text-generation","title":"Basic Text Generation","text":"<pre><code>response = client.run_completions(\n    prompt=\"Write a short poem about artificial intelligence.\",\n    max_tokens=100,\n    temperature=0.7\n)\n</code></pre>"},{"location":"api/text-completions/#structured-output","title":"Structured Output","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass Poem(BaseModel):\n    title: str = Field(..., description=\"Title of the poem\")\n    content: str = Field(..., description=\"Content of the poem\")\n    theme: str = Field(..., description=\"Main theme of the poem\")\n\nresponse = client.run_completions(\n    prompt=\"Write a short poem about artificial intelligence.\",\n    guided_json=Poem.model_json_schema(),\n    max_tokens=200\n)\n</code></pre>"},{"location":"api/text-completions/#error-handling","title":"Error Handling","text":"<pre><code>response, error = client.run_completions(\n    prompt=\"Write a short poem about artificial intelligence.\",\n    return_error=True\n)\n\nif error:\n    print(f\"Error: {error['message']}\")\n    print(f\"Suggestion: {error['suggestion']}\")\nelse:\n    print(response)\n</code></pre>"},{"location":"api/text-completions/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Chat Completions</li> <li>Explore Vision API</li> <li>Check out the OpenAI Examples</li> </ul>"},{"location":"api/vision-api/","title":"Vision API","text":"<p>The Vision API allows you to process images and generate text descriptions or analysis. This is useful for tasks like image description, object detection, and visual analysis.</p>"},{"location":"api/vision-api/#basic-usage","title":"Basic Usage","text":"<pre><code>from gen_ai.factory import create_client\n\n# Create a client\nclient = create_client(\n    provider=\"openai\",\n    api_key=\"your-api-key\",\n    endpoint=\"https://api.example.com/v1\",\n    model_name=\"gpt-4-vision-preview\"\n)\n\n# Process an image\nimage_paths = [\"path/to/image.jpg\"]\nprompt = \"Describe the image in detail.\"\n\nresponse = client.run_vision_request(\n    prompt=prompt,\n    image_paths=image_paths,\n    max_tokens=150\n)\nprint(response)\n</code></pre>"},{"location":"api/vision-api/#streaming","title":"Streaming","text":"<p>For real-time image analysis, use the streaming API:</p> <pre><code># Stream the response token by token\nfor token in client.run_vision_request(\n    prompt=\"Describe the image in detail.\",\n    image_paths=[\"path/to/image.jpg\"],\n    stream=True,\n    max_tokens=150\n):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"api/vision-api/#parameters","title":"Parameters","text":""},{"location":"api/vision-api/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>prompt</code>: Text prompt or message to send with the images</li> <li><code>image_paths</code>: List of paths to images to include in the request</li> </ul>"},{"location":"api/vision-api/#optional-parameters","title":"Optional Parameters","text":"<ul> <li><code>max_tokens</code>: Maximum number of tokens to generate (default: 256)</li> <li><code>temperature</code>: Controls randomness (0.0 to 1.0, default: 0)</li> <li><code>top_p</code>: Controls diversity via nucleus sampling (default: 1.0)</li> <li><code>timeout</code>: Request timeout in seconds (default: 30)</li> <li><code>guided_json</code>: JSON schema to enforce structured output</li> <li><code>return_error</code>: Return error details along with the result</li> <li><code>stream</code>: Whether to stream the response (default: False)</li> </ul>"},{"location":"api/vision-api/#examples","title":"Examples","text":""},{"location":"api/vision-api/#basic-image-description","title":"Basic Image Description","text":"<pre><code>response = client.run_vision_request(\n    prompt=\"Describe the image in detail.\",\n    image_paths=[\"path/to/image.jpg\"],\n    max_tokens=150\n)\n</code></pre>"},{"location":"api/vision-api/#structured-image-analysis","title":"Structured Image Analysis","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass ImageAnalysis(BaseModel):\n    description: str = Field(..., description=\"Detailed description of the image\")\n    objects: list[str] = Field(..., description=\"List of objects in the image\")\n    colors: list[str] = Field(..., description=\"List of colors in the image\")\n\nresponse = client.run_vision_request(\n    prompt=\"Analyze this image.\",\n    image_paths=[\"path/to/image.jpg\"],\n    guided_json=ImageAnalysis.model_json_schema(),\n    max_tokens=300\n)\n</code></pre>"},{"location":"api/vision-api/#chat-with-images","title":"Chat with Images","text":"<pre><code>messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are an expert at image analysis.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What's interesting about this image?\"\n    }\n]\n\nresponse = client.run_vision_request(\n    prompt=messages,\n    image_paths=[\"path/to/image.jpg\"],\n    max_tokens=200\n)\n</code></pre>"},{"location":"api/vision-api/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Text Completions</li> <li>Explore Chat Completions</li> <li>Check out the OpenAI Examples</li> </ul>"},{"location":"examples/advanced-features/","title":"Advanced Features","text":"<p>This guide covers advanced features and techniques for using DevToolkit effectively.</p>"},{"location":"examples/advanced-features/#structured-output","title":"Structured Output","text":""},{"location":"examples/advanced-features/#using-pydantic-models","title":"Using Pydantic Models","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\nclass CodeReview(BaseModel):\n    summary: str = Field(description=\"Brief summary of the code review\")\n    issues: List[str] = Field(description=\"List of issues found\")\n    suggestions: List[str] = Field(description=\"List of improvement suggestions\")\n    score: float = Field(description=\"Overall code quality score from 0 to 10\")\n\n# Generate structured output\nresponse = client.run_chat_completions(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a code reviewer.\"},\n        {\"role\": \"user\", \"content\": \"Review this code: def add(a,b): return a+b\"}\n    ],\n    guided_json=CodeReview\n)\n\n# Access structured data\nreview = CodeReview.parse_raw(response)\nprint(f\"Score: {review.score}\")\nprint(f\"Issues: {review.issues}\")\n</code></pre>"},{"location":"examples/advanced-features/#custom-json-schema","title":"Custom JSON Schema","text":"<pre><code>schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"sentiment\": {\"type\": \"string\", \"enum\": [\"positive\", \"negative\", \"neutral\"]},\n        \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n        \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    }\n}\n\nresponse = client.run_completions(\n    prompt=\"Analyze the sentiment of: 'I love this product!'\",\n    guided_json=schema\n)\n</code></pre>"},{"location":"examples/advanced-features/#advanced-chat-features","title":"Advanced Chat Features","text":""},{"location":"examples/advanced-features/#function-calling","title":"Function Calling","text":"<pre><code>from typing import List, Optional\n\ndef get_weather(location: str, unit: str = \"celsius\") -&gt; str:\n    # Implementation here\n    return f\"Weather in {location}: 20\u00b0{unit}\"\n\ndef get_stock_price(symbol: str) -&gt; float:\n    # Implementation here\n    return 150.25\n\n# Define available functions\nfunctions = [\n    {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the current weather in a location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\"},\n                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n            }\n        }\n    },\n    {\n        \"name\": \"get_stock_price\",\n        \"description\": \"Get the current stock price\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"symbol\": {\"type\": \"string\"}\n            }\n        }\n    }\n]\n\n# Use function calling\nresponse = client.run_chat_completions(\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in New York?\"}],\n    functions=functions\n)\n</code></pre>"},{"location":"examples/advanced-features/#context-management","title":"Context Management","text":"<pre><code>class ConversationManager:\n    def __init__(self, client, max_tokens=1000):\n        self.client = client\n        self.max_tokens = max_tokens\n        self.messages = []\n        self.token_count = 0\n\n    def add_message(self, role: str, content: str):\n        # Estimate tokens (rough approximation)\n        estimated_tokens = len(content.split()) * 1.3\n\n        # If adding this message would exceed the limit, remove oldest messages\n        while self.token_count + estimated_tokens &gt; self.max_tokens and self.messages:\n            removed = self.messages.pop(0)\n            self.token_count -= len(removed[\"content\"].split()) * 1.3\n\n        self.messages.append({\"role\": role, \"content\": content})\n        self.token_count += estimated_tokens\n\n    def get_response(self):\n        response = self.client.run_chat_completions(\n            messages=self.messages,\n            max_tokens=150\n        )\n        self.add_message(\"assistant\", response)\n        return response\n\n# Usage\nmanager = ConversationManager(client)\nmanager.add_message(\"system\", \"You are a helpful assistant.\")\nmanager.add_message(\"user\", \"What is Python?\")\nresponse = manager.get_response()\n</code></pre>"},{"location":"examples/advanced-features/#advanced-vision-features","title":"Advanced Vision Features","text":""},{"location":"examples/advanced-features/#multi-image-analysis","title":"Multi-Image Analysis","text":"<pre><code># Analyze multiple images\nimage_paths = [\n    \"path/to/image1.jpg\",\n    \"path/to/image2.jpg\",\n    \"path/to/image3.jpg\"\n]\n\nresponse = client.run_vision_request(\n    prompt=\"Compare these images and identify common elements.\",\n    image_paths=image_paths,\n    max_tokens=200\n)\n</code></pre>"},{"location":"examples/advanced-features/#image-analysis-with-structured-output","title":"Image Analysis with Structured Output","text":"<pre><code>class ImageAnalysis(BaseModel):\n    objects: List[str] = Field(description=\"List of objects detected\")\n    colors: List[str] = Field(description=\"Main colors present\")\n    scene: str = Field(description=\"Description of the scene\")\n    confidence: float = Field(description=\"Confidence score of the analysis\")\n\nresponse = client.run_vision_request(\n    prompt=\"Analyze this image in detail.\",\n    image_paths=[\"path/to/image.jpg\"],\n    guided_json=ImageAnalysis\n)\n</code></pre>"},{"location":"examples/advanced-features/#advanced-batch-processing","title":"Advanced Batch Processing","text":""},{"location":"examples/advanced-features/#concurrent-processing","title":"Concurrent Processing","text":"<pre><code># Process multiple prompts concurrently\nresponses = client.run_completions_batch(\n    prompts=[\n        \"Explain quantum computing\",\n        \"Describe machine learning\",\n        \"What is blockchain?\"\n    ],\n    concurrent=True,\n    max_concurrent=3,\n    max_tokens=150\n)\n</code></pre>"},{"location":"examples/advanced-features/#batch-with-different-parameters","title":"Batch with Different Parameters","text":"<pre><code># Process prompts with different parameters\nrequests = [\n    {\n        \"prompt\": \"Short answer about Python\",\n        \"max_tokens\": 50,\n        \"temperature\": 0.3\n    },\n    {\n        \"prompt\": \"Creative story about AI\",\n        \"max_tokens\": 200,\n        \"temperature\": 0.8\n    }\n]\n\nresponses = client.run_completions_batch(\n    prompts=[r[\"prompt\"] for r in requests],\n    max_tokens=[r[\"max_tokens\"] for r in requests],\n    temperature=[r[\"temperature\"] for r in requests]\n)\n</code></pre>"},{"location":"examples/advanced-features/#error-handling-and-retries","title":"Error Handling and Retries","text":""},{"location":"examples/advanced-features/#custom-retry-logic","title":"Custom Retry Logic","text":"<pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10)\n)\ndef make_request_with_retry(client, prompt):\n    try:\n        return client.run_completions(\n            prompt=prompt,\n            max_tokens=100,\n            return_error=True\n        )\n    except Exception as e:\n        print(f\"Attempt failed: {str(e)}\")\n        raise\n\n# Usage\nresponse, error = make_request_with_retry(client, \"Hello\")\n</code></pre>"},{"location":"examples/advanced-features/#error-recovery","title":"Error Recovery","text":"<pre><code>def process_with_recovery(client, prompt):\n    try:\n        response = client.run_completions(prompt=prompt)\n        return response\n    except Exception as e:\n        if \"rate_limit\" in str(e).lower():\n            # Wait and retry with exponential backoff\n            time.sleep(5)\n            return client.run_completions(prompt=prompt)\n        elif \"context_length\" in str(e).lower():\n            # Truncate prompt and retry\n            truncated_prompt = prompt[:len(prompt)//2]\n            return client.run_completions(prompt=truncated_prompt)\n        else:\n            raise\n</code></pre>"},{"location":"examples/advanced-features/#related-documentation","title":"Related Documentation","text":"<ul> <li>Explore API Documentation for detailed API information</li> <li>Review OpenAI Examples for fundamental examples</li> </ul>"},{"location":"examples/bedrock_examples/","title":"AWS Bedrock Examples","text":""},{"location":"examples/bedrock_examples/#basic-chat","title":"Basic Chat","text":"<pre><code>from gen_ai.factory import create_client\n\nclient = create_client(\n    provider=\"bedrock\",\n    api_key=\"your_aws_access_key\",\n    model_name=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    region=\"us-east-1\",\n    secret_key=\"your_aws_secret_key\"\n)\n\nmessages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\nresponse = client.run_chat_completions(messages)\nprint(response)\n</code></pre>"},{"location":"examples/bedrock_examples/#vision-api","title":"Vision API","text":"<pre><code>messages = [{\"role\": \"user\", \"content\": \"What's in this image?\"}]\nimage_paths = [\"path/to/image.jpg\"]\nresponse = client.run_chat_completions(messages, image_paths=image_paths)\nprint(response)\n</code></pre>"},{"location":"examples/bedrock_examples/#streaming","title":"Streaming","text":"<pre><code>messages = [{\"role\": \"user\", \"content\": \"Tell me a story\"}]\nfor chunk in client.run_chat_completions_streaming(messages):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"examples/bedrock_examples/#related-documentation","title":"Related Documentation","text":"<ul> <li>Review OpenAI Examples for fundamental examples</li> </ul>"},{"location":"examples/openai_examples/","title":"OpenAI Examples","text":"<p>This guide provides examples of how to use DevToolkit with OpenAI-compatible APIs.</p>"},{"location":"examples/openai_examples/#setup","title":"Setup","text":"<p>First, create a client:</p> <pre><code>from gen_ai.factory import create_client\n\n# Create a client\nclient = create_client(\n    provider=\"openai\",\n    api_key=\"your-api-key\",\n    endpoint=\"https://api.example.com/v1\",\n    model_name=\"gpt-3.5-turbo\"\n)\n</code></pre>"},{"location":"examples/openai_examples/#text-generation","title":"Text Generation","text":""},{"location":"examples/openai_examples/#basic-text-completion","title":"Basic Text Completion","text":"<pre><code># Generate text from a prompt\nresponse = client.run_completions(\n    prompt=\"Write a short poem about artificial intelligence.\",\n    max_tokens=100\n)\nprint(response)\n</code></pre>"},{"location":"examples/openai_examples/#streaming-text-generation","title":"Streaming Text Generation","text":"<pre><code># Stream the response token by token\nfor token in client.run_completions_stream(\n    prompt=\"Write a short poem about artificial intelligence.\",\n    max_tokens=100\n):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"examples/openai_examples/#chat","title":"Chat","text":""},{"location":"examples/openai_examples/#basic-chat","title":"Basic Chat","text":"<pre><code># Chat with the model\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What are the key benefits of modular code?\"}\n]\n\nresponse = client.run_chat_completions(\n    messages=messages,\n    max_tokens=150\n)\nprint(response)\n</code></pre>"},{"location":"examples/openai_examples/#multi-turn-conversation","title":"Multi-turn Conversation","text":"<pre><code># First message\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is Python?\"}\n]\nresponse = client.run_chat_completions(messages=messages)\n\n# Add the response to the conversation\nmessages.append({\"role\": \"assistant\", \"content\": response})\n\n# Follow-up question\nmessages.append({\"role\": \"user\", \"content\": \"How does it compare to JavaScript?\"})\nresponse = client.run_chat_completions(messages=messages)\n</code></pre>"},{"location":"examples/openai_examples/#image-analysis","title":"Image Analysis","text":""},{"location":"examples/openai_examples/#basic-image-description","title":"Basic Image Description","text":"<pre><code># Process an image\nimage_paths = [\"path/to/image.jpg\"]\nprompt = \"Describe the image in detail.\"\n\nresponse = client.run_vision_request(\n    prompt=prompt,\n    image_paths=image_paths,\n    max_tokens=150\n)\nprint(response)\n</code></pre>"},{"location":"examples/openai_examples/#streaming-image-analysis","title":"Streaming Image Analysis","text":"<pre><code># Stream the response token by token\nfor token in client.run_vision_request(\n    prompt=\"Describe the image in detail.\",\n    image_paths=[\"path/to/image.jpg\"],\n    stream=True,\n    max_tokens=150\n):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"examples/openai_examples/#batch-processing","title":"Batch Processing","text":""},{"location":"examples/openai_examples/#process-multiple-prompts","title":"Process Multiple Prompts","text":"<pre><code># Process multiple prompts\nprompts = [\n    \"List 3 benefits of cloud computing.\",\n    \"What is the capital of France?\",\n    \"Provide a short definition of machine learning.\"\n]\n\nresponses = client.run_completions_batch(\n    prompts=prompts,\n    max_tokens=100\n)\n\n# Print results\nfor prompt, response in zip(prompts, responses):\n    print(f\"\\nPrompt: {prompt}\")\n    print(f\"Response: {response}\")\n</code></pre>"},{"location":"examples/openai_examples/#error-handling","title":"Error Handling","text":""},{"location":"examples/openai_examples/#basic-error-handling","title":"Basic Error Handling","text":"<pre><code># Get error details with the response\nresponse, error = client.run_completions(\n    prompt=\"Hello\",\n    return_error=True\n)\n\nif error:\n    print(f\"Error: {error['message']}\")\n    print(f\"Suggestion: {error['suggestion']}\")\nelse:\n    print(response)\n</code></pre>"},{"location":"examples/openai_examples/#batch-error-handling","title":"Batch Error Handling","text":"<pre><code>responses, errors = client.run_completions_batch(\n    prompts=prompts,\n    return_error=True\n)\n\nfor i, (response, error) in enumerate(zip(responses, errors)):\n    print(f\"\\nPrompt {i+1}:\")\n    if error:\n        print(f\"Error: {error['message']}\")\n        print(f\"Suggestion: {error['suggestion']}\")\n    else:\n        print(f\"Response: {response}\")\n</code></pre>"},{"location":"examples/openai_examples/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about Text Completions</li> <li>Explore Chat Completions</li> <li>Check out Vision API</li> <li>See Batch Processing</li> <li>Try Advanced Features</li> </ul>"}]}